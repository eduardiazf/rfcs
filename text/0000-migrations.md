- Start Date: 2019-01-25
- RFC PR: (leave this empty)
- Prisma Issue: (leave this empty)

# Summary

Prisma has a powerful declarative migration system. The declarative nature however has inherent limitations such as providing information which is attached to the state transition from datamodel A to datamodel B.
Therefore this spec proposes to optionally add imperative migrations which will be kept in sync with the declarative datamodel.
These imperative migrations are represented by migration scripts, which live in migration files.
Migration files are being generated by the CLI and used as the source of truth for migrations that will be performed in the future.

# Basic example

## The default case when starting out: No migration files and automatic migrations

Datamodel before

```graphql
type User {
  id: ID! @unique
  name: String!
}
```

Datamodel after

```graphql
type User {
  id: ID! @unique
  name: String!
  address: String!
}
```

```bash
prisma migrate
```

`prisma deploy` will be renamed to `prisma migrate`, as the semantics have changed and we're not necessarily dealing with a server anymore. `prisma migrate` now infers the minimal amount of steps needed to get to the second datamodel.

## Opting in to the migration files workflow

As soon as we want more control, we can opt in to the file-based migrations system.

### Controlling migrations by hand

```bash
prisma migrate generate # creates a new file called `./migrate/MIGRATION_ID.ts`
```

We end up with a file like this:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.createField({
    model: 'User',
    name: 'address',
    type: 'String',
    isRequired: true,
  }),
])
```

If we want to set a migration value programmatically for all existing users, we can achieve that by adjusting the migration:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.createField({
    model: 'User',
    name: 'address',
    type: 'String',
    isRequired: false,
  }),
  m.execute(async ctx => {
    for await (const user of ctx.client.users().$stream()) {
      await ctx.client.updateUser(user.id, {
        address: 'some default address in the universe ' + Math.random(),
      })
    }
  }),
  m.updateField({
    model: 'User',
    name: 'address',
    type: 'String',
    isRequired: true,
  }),
])
```

What we have done here is splitting the creation of a required field into creating an optional field, performing a seeding action on that field and afterwards making the field required. Now we can apply the migration using the following command:

```bash
prisma migrate apply # applies the migration in the database
```

Note that we get full type-safety to write this migration script, even the model name for `updateField` will be fully type-safe using a TypeScript union string type.

### The fast path when opted in

To recap the steps in the previous example, we did the following:

1. `prisma migrate generate` to generate the migration script needed to match the datamodel with the migration scripts
2. If needed, adjust the generated migration script
3. `prisma migrate apply` to add the new migration script to the migration table and change the underlying database schema

If the developer just wants to do a quick additive change, without any data migration, `prisma migrate generate` and `prisma migrate apply` can just be abbreviated with `prisma migrate`.

Therefore the semantics of `prisma migrate` slightly changes in the opted-in case. It additionally creates a migration file.

# Motivation

One of the biggest USPs of the Prisma migration system is the automatic transition between two states of datamodels.
While this approach is good enough for most of the use cases, it has a few disadvantages that we want to tackle with this spec.
The first problem is, that it's neither visible, which steps will eventually be executed, nor is the user able to control these steps. Even if the migrations system is running as intended, this introduces an inherent trust issue as users don't have insights to how migrations are being executed.
Another limitation is, that when a breaking change is performed, such as creating a required field, users of the migrations system must be able to not only migrate the underlying database, but also migrate the data. This could e.g. mean adding default values for a new field for all the records that already exist in the database.
To accomplish this, it must be possible to run data migrations between two migration steps.
Orchestrating these data migrations and running them in the right order is the responsibility of the migration runner.
Therefore Prisma should not impose this burden on the developer, but help running things safe and in the right order, no matter if on a local development machine or in a production CI system.

The biggest difference of this spec to the existing migrations system is, that instead of taking the `datamodel.prisma` file as the source of truth for the actual migration, it's just the source to create migration files. The source of truth for migrations are the migration scripts, which are saved in the migration files that are being generated by the CLI with the help of the migration engine.

# Detailed design

## Contents

- [Terms](#terms)
- [Migration Table](#migration-table)
  - [Multiple Data sources](#multiple-data-sources)
  - [Alternative storage options](#alternative-storage-options)
- [Up and Down](#up-and-down)
- [Modes of operation](#modes-of-operation)
  - [Opting-in and out of file-based migrations](#opting-in-and-out-of-file-based-migrations)
  - [Binary vs server](#binary-vs-server)
  - [Dimensions](#dimensions)
- [The migration loop](#the-migration-loop)
  - [1. Automatic migrations](#1-automatic-migrations)
  - [2. File-based migrations](#2-file-based-migrations)
  - [3. Database introspection](#3-database-introspection)
- [Keeping migrations in lockstep](#keeping-migrations-in-lockstep)
  - [Lockstep happy path](#lockstep-happy-path)
  - [Lockstep conflicts](#lockstep-conflicts)
- [Commands](#commands)
  - [`prisma migrate`](#prisma-migrate)
  - [`prisma migrate generate`](#prisma-migrate-generate)
  - [`prisma migrate generate --data-migration`](#prisma-migrate-generate---data-migration)
  - [`prisma migrate apply`](#prisma-migrate-apply)
  - [`prisma migrate apply --snapshot`](#prisma-migrate-apply---snapshot)
  - [`prisma migrate apply --dry-run`](#prisma-migrate-apply---dry-run)
  - [`prisma migrate apply --next`](#prisma-migrate-apply---next)
  - [`prisma migrate rollback`](#prisma-migrate-rollback)
  - [`prisma migrate rollback --pending`](#prisma-migrate-rollback---pending)
  - [`prisma apply-datamodel`](#prisma-apply-datamodel)
- [Filenames & order of execution](#filenames--order-of-execution)
- [Migration operations](#migration-operations)
  - [Schema operations](#schema-operations)
  - [Data operations](#data-operations)
  - [Low-level database access](#lowslevel-database-access)
  - [Implications of operations](#implications-of-operations)
  - [Migrations which depend on correct data](#migrations-which-depend-on-correct-data)
  - [Breaking the API](#breaking-the-api)
  - [Potential rejection based on incorrect data](#potential-rejectiong-based-on-incorrect-data)
  - [Potential rollback](#potential-rollback)
  - [Destructive operations (potential data loss)](#destructive-operations-potential-data-loss)
  - [Informing the user](#informing-the-user)
- [Workflows](#migration-workflows)
  - [Using data migrations to introduce required fields](#using-data-migrations-to-introduce-required-fields)
  - [Git-based team workflow: Compatible Changes ](#git-based-workflows-conflict-resolution)
  - [Git-based team workflow: Incompatible Changes ](#git-based-workflows-conflict-resolution)
  - [Introducing uniqueness in staging and production](#introducing-uniqueness-in-staging-and-production)
  - [Catching breaking changes in the application](#catching-breaking-changes-in-the-application)
- [Limits of migration inference](#limits-of-migration-inference)
- [Squashing & Snapshots](#squashing--snapshots)
  - [Requirements](#requirements)
- [Data Migrations vs Seeding](#data-migrations-vs-seeding)
- [Rollbacks](#rollbacks)
- [Rerunning Migrations](#rerunning-migrations)
- [Opting out of Prisma's migration system](#opting-out-of-prismas-migration-system)

## Terms

The following non-trivial terms will be used across this document:

- [Migration Operation & Migration Step](#migration-operation--migration-step)
- [Migration Table](#migration-table)
- [Migration Engine](#migration-engine)
- [Breaking vs destructive vs potential rejection vs potential rollback](#implications-of-operations)
- [Lockstep](#keeping-migrations-in-lockstep)

## Migration Engine

```
┌────────────┐               ┌────────────────┐
│            │ prisma deploy │ Prisma Server  │         .───────────.
│ Prisma CLI ├─────────────▶ │ Management API ├──────▶ (  Database   )
│            │   datamodel   │                │         `───────────'
└────────────┘               └────────────────┘


┌────────────┐               ┌────────────────┐
│            │ prisma migrate│                │         .───────────.
│ Prisma CLI ├─────────────▶ │Migration Engine├──────▶ (  Database   )
│            │   datamodel   │                │         `───────────'
└────────────┘               └────────────────┘
```

There are two fundamental parts to the migration system: The Prisma CLI and the Prisma migration engine.
The Prisma CLI will orchestrate local migration files and provides access to commands.

The following logic is being implemented in the Migration Engine:

- Inferring migration steps
- Introspecting the database, to check, if specific types exist and the datamodel is in sync with the actual data structure
- Keeping migration scripts and and the migration table in sync

The migration engine is implemented in Rust and embedded into the various run times as Node.js or Go as a binary.

## Migration Operation & Migration Step

This is an example for a migration step:

```ts
m.createField({
  model: 'User',
  name: 'name',
  type: 'String',
})
```

If we talk about a migration operation, we talk about the type of migration, for example `createField` or `updateModel`.
The migration step refers to a concrete instance of such a migration operation, which is part of a migration.
A migration is defined as an array of migration steps, which are each instances of migration operations.

## Migration Table

An important primitive, which the Prisma migrations system has already been using before the creation of this spec, is the Prisma schema, which lives in the same database as the actual data. It turned out, that this Prisma schema only includes one table, the migration table. So we're changing the requirement to only need one table, which can live in the same schema as the data.
This is a table, which exists in the database that the migrations are being executed on. It consists of the history of migrations, that have already been executed and the currently running migration. Only one migration can run at the same time.
These are the benefits of the migration table:

- By setting a lock on the table, it prevents two migrations from being executed at the same time
- It provides the migration history, which can be visualized and useful to understand the evolution of the database schema
- It makes the safe execution of migration steps possible by ensuring that migration steps don't get executed twice.
- It ensures safe rollback behavior from migrations that fail. The migration and its steps are being stored here, so that all steps of a migration that have been executed until then can be undone.

### Multiple data sources

In the future, Prisma will support multiple data sources at the same time. You then can e.g. have a relation between a type `User` that is persisted in Postgres with a type `Post` that is persisted in MySQL.
If such a relation will be introduced, Prisma needs to resolve, which type comes from which data source.
In order to do this, type names need to be globally unique. If that is the case, an ordinary `createRelation` migration operation is sufficient to perform that migration.

All migrations of a Prisma project are stored in one migration table. The question is, where this table is being stored. This will be configurable through the `prisma.yml` file - it could even be stored in a separate database, which doesn't contain the data.

### Alternative storage options

In the version of Prisma of time of writing, the migration table is being stored in the same database as the data lives in. This doesn't have to be the case. Other possible options are the following:

- A central registry hosted by Prisma
- A file in the filesystem
- A file in S3
- A file in Google Cloud Storage
- A table in a different database, which is not being used for the actual data

While these use-cases can enable new workflows, in this spec we're focussing on the migration table that lives in the same database as the data that is being accessed by Prisma.
The reason you may want to have these files in separate storages is, that if you need to share your migrations scripts with other parts of the company, but can't afford to share the whole codebase due to security reasons.

## Up and Down

The avid reader will notice over the course of reading this spec, that we don't talk much about `up` and `down`, which is an integral part of many existing migration systems outside of Prisma.
Let's discuss, why `up` and `down` is _mostly_ not needed in the Prisma migrations system and what the exceptions are.
In many migration systems as [Flyway](https://flywaydb.org/) or [Go Migrate](https://github.com/golang-migrate/migrate), users provide both the actual migration (aka `up`) and the reversal of the migration (`down`).
If we for example create a table in up, the reversal (`down`) is to remove the table:

`up`

```sql
CREATE TABLE X
```

`down`

```sql
DROP TABLE X
```

The `down` to the `up` is needed to rollback migrations.

Providing the reversal by hand is only needed, if the migration system can't deduce it automatically.
As the Prisma is aware of the semantics of all migration steps, Prisma is able to reverse them automatically.

There are exceptions to this, when Prisma is not able to deduce the reversal of a migration.
This is the case, when custom database-specific low-level migrations such as `runSql` or `runMongo` are being defined by the user. As arbitrary, complex SQL queries could be provided here, Prisma is not supporting the automatic reversal here. These, however shouldn't be necessary in the 99% use-case. The Prisma migrations system even exists for over 2 years by now without having these escape hatches, while adding more support for database-specific primitives like indexes.

## Modes of operation

### Opting-in and out of file-based migrations

To keep the current ease-of-use and minimal getting started experience, users will be able to start a new Prisma project without the need to know the concept of migration files.
Prisma will migrate from datamodel A to datamodel B with the same mechanics which power the migrations today - automatically determining the steps needed to perform the migration. As soon as the user hits the use-case to enter the file-based migrations system, a simple `prisma migrate generate` will opt the user in to the file-based migration system. From this point on, for each `prisma migrate` new files will be created into a folder called `./migrate` (which is configurable), relative to the `prisma.yml` file. To opt out of this workflow again, the user needs to delete the `./migrate` folder. From there on, the automatic migrations will be active again.

> The difference between the automatic migrations and the file-based migrations is, that automatic migrations don't require to be [in lockstep with the migration table](#keeping-migrations-in-lockstep).

### Binary vs server

Another dimension that plays a role for migrations is the mode in which Prisma runs. Prisma in the future will both be available as a binary that can be shipped as a library in specific languages and also a stand alone server, that can be deployed as a separate infrastructure component.
In which mode Prisma is running, has an influence on how the introspection flow looks like. The main relevant difference for migrations is, that the binary gets the datamodel that it takes as a basis to expose the GraphQL API injected on boot time.
With this approach, the binary is representing a single-tenant approach, where per running binary only one datamodel can be exposed as an API. In contrast to that, the server gets the datamodel injected in runtime and is able to serve multiple Prisma projects at the same time.
This means for migrations, that the database migrations doesn't change, no matter if Prisma binary or Prisma server is being used to access the data.
The action of applying the datamodel however, differs.
While the binary will get the datamodel injected on boot time, the Prisma server will need to set the datamodel for a project with a specific action. We call that command `prisma apply-datamodel`.

### Dimensions

Taking these 2 dimensions of opting in or out of the file-based migrations and using a Prisma binary or server thereby represent 4 different modes in which the migration system can be operated.

## The migration loop

The following diagram describes 3 different workflows around migrations:

```
┌──────────────────┐      ┌─────────────────┐
│  migration files ├─────▶│ migration table │
│                  │      └──────────┬──────┘
│┌────────────────┐│       ▲         │
││ migration-1.ts ││       │         │
│└────────────────┘│       │         │
│┌────────────────┐│       │         ▼
││ migration-2.ts ││       │   .───────────.
│└────────────────┘│       │  (  database   )
│┌────────────────┐│ ┌─────┘   `─────┬─────'
││ migration-3.ts ││ │               │
│└────────────────┘│ │               │
└──────────────────┘ │               │
          ▲          │               │
          │          │               ▼
┌─────────┴──────────┤       ┌───────────────┐
│  datamodel.prisma  │ ◀─────┤ introspection │
└────────────────────┘       └───────────────┘
```

### 1. Automatic migrations

When starting out with a fresh Prisma project, the user just describes the datamodel in a declarative manner into the `datamodel.prisma` file. When executing `prisma migrate`, this datamodel is being sent to the migration engine and the migration engine will calculate the necessary steps, if any, to migrate the underlying database to the new datamodel. This is how Prisma works as of the time of writing this spec.

### 2. File-based migrations

As soon, as a use-case is hit, where the Prisma user needs more control over the migrations system, such as adding a required field, the user can opt in to the migrations system with `prisma migrate generate`.
In the case, that already 10 migrations are stored in the migration table, these 10 migration files will be created.
Let's call the datamodel, that results from applying all migration files to an empty database the "migration file datamodel". If the migration file datamodel and the datamodel.prisma should differ, the migration engine now calculates the steps needed to get from the migration file datamodel to the datamodel.prisma, in other words, the diff.
This diff is now saved to a new migration file.
A single `prisma migrate` will now send all migrations to the migration engine.
The migrations engine can decide based on the current actual database schema and the migration table, which migrations should be executed.

### 3. Database introspection

A workflow that is not related to migrations per se, is the database introspection.
When starting out with a new Prisma project, there are two modes of operation:

1. Starting with an empty database and migrating the database to the datamodel declared in the `datamodel.prisma` file.
2. Starting with an existing database and creating a datamodel that maps to the existing database schema by using `prisma introspect`

As visible in the diagram above, the introspection now closes the loop when it comes to creating datamodels and migrating these to databases. It allows powerful workflows as incrementally adopting Prisma into an existing system while still keeping the existing migrations system. Prisma merely needs to re-introspect the database to obtain the new datamodel.

## Keeping migrations in lockstep

> File-based migrations require the migrations to be in [lockstep](<https://en.wikipedia.org/wiki/Lockstep_(computing)>) with the steps saved in the migration table.

### Lockstep happy path

So if the local migration files have the following migrations:

```
Migration 1: Steps a,b,c
Migration 2: Steps d,e
Migration 3: Steps f,g
```

and the migration table the following entries

```
Migration 1: Steps a,b,c
Migration 2: Steps d,e
```

the migration engine will determine, that `Migration 3` is new and needs to be executed.
In contrast to that, the automatic migrations don't this syncing and just calculate the diff of the datamodel in the local filesystem and the migration table.

### Lockstep conflicts

Lockstepping has the effect, that the migration engine will reject any migration based on migration files, if the local migration files are out of sync with the remote migration table. This case can only occur, if the user manually changes the migration script.
Prisma will generate a useful error message stating the exact difference of the migrations.
The user can do 3 things from here:

1. Fix the migrations based on the feedback the migration engine gave.
2. Delete or rename the migrations folder, execute `prisma migrate generate` to get the migrations stored in the migration table.
3. Use `prisma migrate apply --force-overwrite`, which overrides the whole migration history in the migration table.
4. Use `prisma migrate apply --ignore-migrations` to deploy based on the latest datamodel. This has the same behavior as if the user would be opted out of the migrations system, pretending as if there were no migration files.

## Commands

### `prisma migrate`

This is the command, users use when starting out with a fresh project.
In the beginning it won't create any migration files.
Later, when opted in to the file-based migrations, it will perform:
`prisma migrate generate` and `prisma migrate apply`, but not `prisma apply-datamodel` for the server case.
This differs from the behavior of the old `prisma deploy` command. There the equivalent of `prisma apply-datamodel` was automatically executed.

What happens when `prisma migrate` is being executed:

```
┌─┬────────────┐  inferMigrationSteps(     ┌────────────────┐
│1│ prisma-cli │       datamodel)          │migration engine│
└─┴────────────┘──────────────────────────▶└────────────────┘

┌─┬────────────┐   startMigration(steps,   ┌────────────────┐                   ┌───────────────┐
│2│ prisma-cli │        datamodel)         │migration engine│ checkSteps(steps) │migration table│
└─┴────────────┘──────────────────────────▶└────────────────┘──────────────────▶└───────────────┘

┌─┬────────────┐                           ┌────────────────┐                     .───────────.
│3│ prisma-cli │ applyNextMigrationStep()  │migration engine│  applyStep(step)   (  database   )
└─┴────────────┘──────────────────────────▶└────────────────┘──────────────────▶  `───────────'

┌─┬───────────────────────────────────┐
│4│ Repeat 3 until all steps are done │
└─┴──────────────────────────────── ◀─┘

```

### `prisma migrate generate`

Create migration files based on the steps that the migration engine calculates

### `prisma migrate generate --data-migration`

Create a template file to implement a data migration, which gets the Prisma Client injected.

### `prisma migrate apply`

Applies migrations in the database

### `prisma migrate apply --snapshot`

In the case that an application already has many migration files, which would need a long time to be executed from scratch, this command can be used to "squash" all steps to the minimum amount of steps needed to get from an empty database to the latest datamodel that is being provided in the application. This is especially interesting in CI environments, in which speed of setting up a new Project is important.

Read more about this and how this effects data migrations and seeding in the [Snapshots](#snapshots) chapter.

### `prisma migrate apply --dry-run`

Executes the migrations in a dry-run and informs about potential problems as illegal migrations or destructive change

### `prisma migrate apply --next`

Instead of executing all unapplied migrations, only the pending migrations (which are still running) and the next unapplied migration will be executed.

### `prisma migrate rollback`

Rolls the last successful migration back.

### `prisma migrate rollback --pending`

Rolls the last pending migration back, if there is any.
A migration can get into the pending state, if it got interrupted, for example though internet problems.

### `prisma apply-datamodel`

Only update the API Schema, but not the underlying database

## Filenames & order of execution

Prisma has an opinionated naming scheme for migration file.

`TIMESTAMP-script.ts`, where TIMESTAMP is a unix timestamp. The order in which Prisma reads the migration files is determined by a lexicographical ordering of the file names.

Each migration both has a language specific file including the migration script and a corresponding snapshot of the datamodel:

```
1551279957819-script.ts
1551279957819-datamodel.prisma
```

## Migration Operations

### Schema operations

A detailed list of all schema manipulation operations that Prisma will provide [can be found here](https://gist.github.com/mavilein/5b4a7407a4dfde9c070d26f276b0aa13)

All migration operation, that Prisma ships out of the box are reversible. That means that Prisma can for a migration step `createField` automatically infer the `deleteField` reversion of the operation.
Therefore, every migration operation can both be used explicitly in the migration script in the `up` part of a migration or implicitly perfomed by Prisma in the unlikely event of a rollback.

### Data operations

One of the main reasons to open up the migrations system is to allow data migrations.
In TypeScript, they can be defined using the `execute` operation:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.createField({
    model: 'User',
    name: 'address',
    type: 'String',
    isRequired: false,
  }),
  m.execute(async ctx => {
    for await (const user of ctx.client.users().$stream()) {
      await ctx.client.updateUser(user.id, {
        address: 'some default address in the universe ' + Math.random(),
      })
    }
  }),
  m.updateField({
    model: 'User',
    name: 'address',
    type: 'String',
    isRequired: true,
  }),
])
```

The context object `ctx` and the `client` property will typed. This will allow for type safe data migrations that can be run in between two migration steps are being executed.
When the 3 steps defined here will be sent to the migration engine, the `.execute` step will be replaced by a no-op.
The advantage is, that the number of steps in the migration script and in the migration table will be equal, therefore it will be easier to ensure the lockstep between migration script and migration table.
Important to note: In this example we have the `m.execute` operation in the same file as some schema manipulation operations.
When in production, you probably want to keep them in separate files, as you otherwise can't adjust your application in the meantime. We will discuss how this workflow will exactly look like in [Using data migrations to introduce required fields](#using-data-migrations-to-introduce-required-fields).

All built-in migration operations that Prisma ships, like `m.createField`, are reversible (some with destructive change), because Prisma knows the semantics of each operation. That means in the case of a rollback, Prisma is able to revert any migration operation.

When custom `m.execute` functions are provided though, Prisma can't infer the reversion as arbitrary code could have been run.
If the user wants to be able to revert the actions that have been executed in a `m.execute` operation, she can provide the reversal using an object with an `up` and `down` property just like so:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.createField({
    model: 'User',
    name: 'address',
    type: 'String',
    isRequired: false,
  }),
  m.execute({
    up: async ctx => {
      for await (const user of ctx.client.users().$stream()) {
        await ctx.client.updateUser(user.id, {
          address: 'some default address in the universe ' + Math.random(),
        })
      }
    },
    down: async ctx => {
      await ctx.client.updateManyUsers({
        data: {
          address: null,
        },
      })
    },
  }),
  m.updateField({
    model: 'User',
    name: 'address',
    type: 'String',
    isRequired: true,
  }),
])
```

In cases that the `up` data migration is migrating data for a field that just has been created as in the example above, a reversal is not needed, as reverting `createField` is a destructive change anyways and will end up with a schema without that particular field.

### Low-level database access

#### SQL

To allow manipulating the underlying database directly, we provide a SQL interface:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [m.runSql({ up: `CREATE TABLE X`, down: `DROP TABLE X` }), m.runSql(`CREATE TABLE Y`)])
```

Users can optionally provide the `down` step to be able to reverse the migration.

#### MongoDB

An underlying MongoDB database can be manipulated like this:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.runMongo({
    up: [
      {
        createIndexes: 'mycollection',
        indexes: [
          {
            key: {
              username: 1,
              created: -1,
            },
            name: 'username_sort_by_asc_created',
            background: true,
          },
          {
            key: {
              email: 1,
            },
            name: 'unique_email',
            unique: true,
            background: true,
          },
        ],
      },
    ],
    down: [
      {
        dropIndexes: 'mycollection',
        index: 'username_sort_by_asc_created',
      },
      {
        dropIndexes: 'mycollection',
        index: 'unique_email',
      },
    ],
  }),
])
```

### Implications of operations

This is an overview which operations may cause certain effects

| **Operation**                       | **Breaking in API** | **Potential rollback** | **Potential Rejection based on incorrect data** | **Destructive (Potential data loss)** |
| ----------------------------------- | ------------------- | ---------------------- | ----------------------------------------------- | ------------------------------------- |
| CreateModel                         |                     | ✅                     |                                                 |                                       |
| DeleteModel                         | ✅                  | ✅                     |                                                 | ✅                                    |
| UpdateModel - rename model          | ✅                  | ✅                     |                                                 |                                       |
| CreateField                         |                     | ✅                     |                                                 |                                       |
| DeleteField                         | ✅                  | ✅                     |                                                 | ✅                                    |
| UpdateField - rename field          | ✅                  | ✅                     |                                                 |                                       |
| UpdateField - make field required   | ✅                  | ✅                     | ✅                                              |                                       |
| UpdateField - change type           | ✅                  | ✅                     | ✅                                              | ✅                                    |
| UpdateField - list <> non list      | ✅                  | ✅                     |                                                 | ✅                                    |
| UpdateField - make field unique     |                     | ✅                     | ✅                                              |                                       |
| CreateEnum                          |                     | ✅                     |                                                 |                                       |
| DeleteEnum                          | ✅                  | ✅                     |                                                 |                                       |
| UpdateEnum - rename enum            | ✅                  | ✅                     |                                                 |                                       |
| UpdateEnum - add value              |                     | ✅                     |                                                 |                                       |
| UpdateEnum - remove value           | ✅                  | ✅                     | ✅                                              |                                       |
| CreateRelation                      |                     | ✅                     |                                                 |                                       |
| DeleteRelation                      | ✅                  | ✅                     |                                                 | ✅                                    |
| UpdateRelation - Rename             |                     | ✅                     |                                                 |                                       |
| UpdateRelation - Cardinality change | ✅                  | ✅                     | ✅                                              | ✅                                    |
| UpdateRelation - add Field          |                     | ✅                     |                                                 |                                       |
| UpdateRelation - remove Field       | ✅                  | ✅                     |                                                 |                                       |

In the following we discuss the details of these implications.

### Breaking in API

This includes any change as removing or renaming a field, which could potentially break existing application code.

### Potential Rejection based on incorrect data

The following migrations can not be performed, if the actual data in the database doesn't have the right structure.
Prisma checks for the data validity before performing the actual migration, but between the check and the actual migration, there still could be records inserted that are making the migration fail.

#### `updateField` - make field required

If a field already exists and a few records have the value `null` for this field, it is not possible to set that field to required.

#### `updateRelation` - make relation required

If a 1:1 or 1:n has been optional in the to 1 part and should be turned into a required relation, all records have to be scanned in the database to ensure, that this constraint is being fulfilled.

Example:

```graphql
type User {
  id: ID! @id
  posts: [Post!]!
}

type Post {
  id: ID @id
  user: User
}
```

If `user` will now become required, the datamodel will be updated like this:

```graphql
type User {
  id: ID! @id
  posts: [Post!]!
}

type Post {
  id: ID @id
  user: User!
}
```

Now all `Post`s already need to have a user associated. If that is not the case, the migration can't be performed.

#### `updateRelation` - turn n:1 into 1:1 relation

There are two ways to define 1:n relation in Prisma:

1. Explicitly set the back relation

```graphql
type User {
  id: ID! @id
  posts: [Post!]!
}

type Post {
  id: ID @id
  user: User
}
```

2. Don't provide the back relation

```graphql
type User {
  id: ID! @id
}

type Post {
  id: ID @id
  user: User
}
```

In both cases, Prisma will create a 1:n relation between User and Post.
If now an explicit `to one` back relation will be added from User:

```graphql
type User {
  id: ID! @id
  post: Post
}

type Post {
  id: ID @id
  user: User
}
```

This now means that for each User there must only be one Post and vise versa.
Depending on the current configuration of actual relations in the database, this migration operation may get rejected.

#### `unique`

If a unique constraint will be applied on a field of a model that already has data, no duplicates are allowed.

#### `updateEnum` - removing an enum value

If there is a data record, which still uses the enum value that is going to be removed, the migration will get rejected.

### Potential rollback

#### 3rd party changes to the database schema

Indeed any operation can cause a rollback.
The cause for this could be, that just before the operation will be performed and after Prisma has checked the data for validity, the underlying database schema may have been changed from another source, which is not part of the Prisma project.
This is a very unlikely event and should be prevented by the database admin by not giving access to schema migrations to third parties.
Prisma introspects the database once and creates in internal datamodel representation from that.
From that point on, all migrations are based on the virtual datamodel representation Prisma has.

#### Changes to the data after the validity check

In the same manner as changes to the database schema can cause rollbacks, this can also happen with changes to data records, that are stored in the database. Right after a unique check has been performed on a specific field, there could be a quick insert in between the validity check of the database and the actual migration.

> Both problems can generally be prevented by locking the whole database for the time of the migration. This is an easy fix for the situation, but obviously not affordable for all application.

The solution for failed migrations in general is very often adding a `m.execute` operation right before the migration will be executed, to ensure that the data is in the right shape.

### Destructive operations (potential data loss)

All migration operation, that Prisma ships out of the box are reversible. That means that Prisma can for a migration step `createField` automatically infer the `deleteField` reversion of the operation.
Some operations can cause data loss. We call such operations destructive.

The following operations are destructive and should therefore be use with care. These are the typical "scary" operations, which in some production systems are even not used anymore, as depending on the application the implications can be hard to reason about.

#### `deleteField`

If a field is being removed, the data that has been stored in that field will automatically also be deleted.

#### `deleteModel`

If a model is being removed, the data that has been stored in that model, all fields and relations will automatically also be deleted.

#### `updateField` which changes the data type

The following changes in a field will cause data loss:
As foreign key columns already have an index defined, this operation should be fairly quick even on big data sets.

- Change of scalar type: If the scalar type e.g. changes from `String` to `Int`, Prisma will clear the data. The user will just be able to provide a general default value for these fields.
- Change from list to non-list or vise-versa

#### `updateRelation`

- Change of cardinality (e.g. 1:1 to 1:n). This will be fixed in a future version of Prisma by actually migrating the data records. As foreign keys are indexed anyways, this operation should be fairly fast.
- Even if the cardinality stays the same, a change of the concrete persistence of the relation (Link Table or Inline) also will result in data loss right now. This is exactly the same use-case as the change of cardinality.

### Informing the user

In order to ensure successful migrations, Prisma performs certain checks at specific points in the migration flow.
These are the checks Prisma performs, which in the failure case immediately get communicated to the user:

#### `prisma migrate generate`

1. Validate the new datamodel
2. Validate that local migrations and migrations in the migration table are in sync
3. Infer the necessary migration steps
4. Check for each non-performed migration step:
   1. Is the step possible with the current data? Are all records e.g. adhering to a unique step that will be introduced?
   2. Is the step potentially destructive? - Could there be data loss?
   3. Could the step be breaking the API of the Prisma Client? If yes - inform the user.

#### `prisma migrate apply`

1. Validate that local migrations and migrations in the migration table are in sync
2. Check for each non-performed migration step:
3. Is the step possible with the current data? Are all records e.g. adhering to a unique step that will be introduced?
4. Is the step potentially destructive? - Could there be data loss?
5. If the Client API is breaking or not is not interesting here anymore.

If the migration engine realizes during the `prisma migrate apply` command, that there are destructive changes,
the user will be required to confirm them. This can either happen interactively if supported by the environment, or especially in CI
environments the confirmation can be provided with a `--force` flag as a CLI arg.

The CLI output for the interactive will look like this:

```
You are going to perform destructive changes (with potential data loss).
Do you want to proceed? (y/N)
```

## Workflows

### Using data migrations to introduce required fields

The new migrations workflow will look like this:

A user initiates a new project with `prisma init` and ends up with a folder structure like this:

```
.
├── datamodel.prisma
└── prisma.yml
```

and a `datamodel.prisma` file like this:

```graphql
type User {
  id: ID! @id
  email: String! @unique
  name: String!
}
```

The user then executes the `prisma migrate` command. This command sends the datamodel to the migration engine, which then automatically migrates the database to the new datamodel.

Every time the user does changes in the datamodel.prisma, a simple `prisma migrate` updates the underlying database schema. This is how migrations with Prisma work as of the time of writing.

Let's say the developer using Prisma now runs the application in production and 100,000 users register for the app.
She finds out, that she wants to turn the single `name` field into a `firstName` and `lastName` field, which both ultimately should be required.

The migration of the database could be solved with the following steps both in the old and new migrations system:

1. Introduce the `firstName` and `lastName` field for the `User` type, run `prisma migrate`.
2. Update the application code to use the `firstName` and `lastName` fields when creating new users.
3. Create and run a script, which migrates all existing users to the new structure.
4. Remove the `name` field from the datamodel, turn the `firstName` and `lastName` into required fields.
   Important to note here: The application must not point to the input types of `firstName` and `lastName` directly, as they would break now.

While creating and running the data migration script of step 3 is probably fairly easy to accomplish, a lot of effort with orchestration on top is needed to execute steps 1-4 in order in any other system as CI.
As soon as these data migrations are introduced multiple times, the situation gets even worse.
This is where the new Prisma file-based migrations come into play. To accomplish steps 1-4 with the file-based migrations, the following concrete steps are needed:

#### 1. Add `firstName` and `lastName` to the `datamodel.prisma` file:

```graphql
type User {
  id: ID! @id
  email: String! @unique
  name: String!
  firstName: String
  lastName: String
}
```

#### 2. Run `prisma migrate generate`. This creates 2 migration files: The first one including the initial migration we already ran, the second one representing the change of adding the new two fields:

`1551443000000-script.ts`

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.createModel({
    model: 'User',
    fields: {
      id: { name: 'id', type: 'ID' }
      name: { name: 'name', type: 'String' }
    }
  })
])
```

`1551444000000-script.ts`

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.addField({
    model: 'User',
    name: 'firstName',
    type: 'String',
    isRequired: false,
  }),
  m.addField({
    model: 'User',
    name: 'lastName',
    type: 'String',
    isRequired: false,
  }),
])
```

#### 3. Run `prisma migrate apply` to add the fields to the database schema

The migration engine will recognize, that `1551443000000-script.ts` has been executed already and will execute the steps provided in `1551444000000-script.ts`.

#### 4. Update the application code to use the `firstName` and `lastName` fields when creating new users. (Step 2)

#### 5. Run `prisma migrate generate --data-migration` to bootstrap the data migration script we will need split `name` into `firstName` and `lastName`.

The content of this file will look like this:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.execute(async ctx => {
    // access the client in a type-safe matter: ctx.client.
  }),
])
```

We now can implement our type-safe data migration:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.execute(async ({ client }) => {
    for await (const users of client
      .users()
      .$stream({ batch: true })
      const batch = users.map(user => {
        const [firstName, lastName] = user.name.split(/\s+/)
        return ctx.prisma.users.update({
          where: user.id,
          data: {
            name: null,
            firstName,
            lastName,
          },
        })
      })
      await ctx.prisma.batch(batch)
    }
  }),
])
```

By running `prisma migrate apply`, the Prisma CLI will now run the data migration. The migration engine will add a _noop_ migration into the migration table to mark this migration as executed.

#### 6. Now we can remove the `name` field and make our new fields required:

```graphql
type User {
  id: ID! @id
  email: String! @unique
  firstName: String!
  lastName: String!
}
```

By running `prisma migrate`, a new migration file will be created and the migration will immediately executed.
The filesystem will now look like this:

```
.
├── datamodel.prisma
├── migrate
│   ├── 1548425145186-script.ts
│   ├── 1548425145186-datamodel.prisma
│   ├── 1548425240000-script.ts
│   ├── 1548425240000-datamodel.prisma
│   ├── 1548425340000-data-migration-script.ts
│   └── 1548425450000-script.ts
│   └── 1548425450000-datamodel.prisma
└── prisma.yml
```

### Git-based team workflow: Compatible Changes

Bob checks out the code locally and both Jen and Bob have a local version of Prisma running with their own database.
Both Bob and Jen now perform a change to the datamodel, a local Migration file will be created and Jen pushes first.
Bob doesn't pay too much attention which code Jen already pushed, just pulls and as he doesn't get any conflict, also pushes his code:

Jen's new `1548425150000-script.ts`:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.createField({
    model: 'User',
    name: 'address',
    type: 'String',
    migrationValue: '',
  }),
])
```

Bob's new `1548425159999-script.ts`

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.createField({
    model: 'User',
    name: 'age',
    type: 'Int',
    migrationValue: -1,
  }),
])
```

After the merge, the filesystem will look like this:

```
.
├── datamodel.prisma
├── migrate
│   ├── 1548425145186-script.ts
│   ├── 1548425145186-datamodel.prisma
│   ├── 1548425140000-script.ts
│   ├── 1548425140000-datamodel.prisma
│   ├── 1548425149999-script.ts
│   ├── 1548425149999-datamodel.prisma
└── prisma.yml
```

The resulting datamodel will look like this:

```graphql
type User {
  id: ID! @id
  name: String!
  address: String!
  age: Int!
}
```

The migration engine will realize, that Jen's changes have already been applied and will as usual add Bob's migration.
The assumption here is, that merge conflicts always occur with one of the changes already merged in.

### Git-based team workflow: Incompatible Changes

Let's assume Jen now removes the `name` field from the `User` type, but Bob just wants to make it optional. This is a change that can't be solved automatically - we have a merge conflict.

The first barrier to prevent this from happening is a Git-based merge conflict in the `datamodel.prisma` file:

Jen's `datamodel.prisma` will look like this:

```graphql
type User {
  id: ID! @id
}
```

While Bob's `datamodel.prisma` will look like this:

```graphql
type User {
  id: ID @id
  name: String
}
```

The next barrier to catch this conflict is the actual Prisma migration system.

When the `prisma apply-migration` command is being executed in CI, it will read all migration files.

As soon as it will see the following two migrations, it will recognize a conflict:

Jen's migration file:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.deleteField({
    model: 'User',
    name: 'name',
  }),
])
```

Bob's migration file:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.updateField({
    model: 'User',
    name: 'name',
    type: 'String',
    isRequired: false,
  }),
])
```

The migrations system will be able to recognize, that in order to execute `updateField`, `createField` is a prerequisite.

### Introducing uniqueness in staging and production

This example discusses the challenges of keeping two Prisma projects as a staging and production environment in sync and why we need the `prisma rollback` command in failure cases.

Let's say we have the following datamodel

```graphql
type User {
  id: ID! @id
  name: String!
  email: String!
}
```

We now realize, that we want to add a unique constraint to the `email` field. After applying the constraint, our datamodel looks like this:

```graphql
type User {
  id: ID! @id
  name: String!
  email: String! @unique
}
```

The inferred migration script to accomplish this change is:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.updateField({
    model: 'User',
    name: 'email',
    isRequired: true,
    unique: true,
  }),
])
```

Now when running `prisma generate apply`, the migration engine will check in our local Prisma project, if all records full-fill the unique constraint.
If not, the migration will not even be applied and the User needs to implement a script.
Let's assume, that the migration runs locally just fine.

Now we push our change to Github and a CI system picks up the changes.
The CI system will run `prisma migrate apply` and let's again assume that this goes fine and all records in the staging database full-fill the unique constraint.
The CI build will be green and looks good to merge into production, so we press the merge button.
Again, `prisma migrate apply` will be executed and the migration engine will check against the production database, if the dataset is duplicate free for the `email` field. This now fails. Prisma finds a conflict in one record and doesn't even apply the migration.
The production CI build therefore also fails.

What now?

We would now see, that our production build failed and would see a message from the Prisma CLI in the CI logs, that the unique constraint was validated.
We now have two options to accomplish our unique constraint:

1. Adjust the production data by hand
2. Add a data migration step in our migration script, that removes any duplicates.

Option 1 is outside of the scope of this spec, so let's discuss option 2.

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.exec(async ctx => {
    const userIds = []
    const duplicates = []
    for await (const user of ctx.prisma.users().$stream()) {
      if (userIds.includes(user.id)) {
        duplicates.push(user.id)
      } else {
        userIds.push(user.id)
      }
    }
    await ctx.prisma.deleteUsers(userIds)
  }),
  m.updateField({
    model: 'User',
    name: 'email',
    isRequired: true,
    unique: true,
  }),
])
```

If we now push and execute `prisma generate apply` in the staging CI system, it will fail, because this migration has already been executed in staging and we require a lockstep between migration scripts and the migration table.
In order to fix this problem, we will now manually have to run `prisma migrate rollback` for the staging environment.

This isn't a nice user experience, but a tradeoff that we have to make right now in order to preserve the lockstep.

Another option here would be an equivalent to `git push --force`, which would ignore the lockstep.
This however is not recommended as the guarantees that the migration table provides us would be gone.
As soon as staging ran back the last migration, it's again in the same state as the production system. We can now push this adjusted migration and successfully add the unique constraint to the `email` field.

Adding a `unique` constraint is not the only migration operation, which relies on the right data. [Read more here](#implications-of-operations) about all other migration steps which also rely on the correct data.

### Catching breaking changes in the application

Let's say our application has the following datamodel:

```graphql
type User {
  id: ID! @id
  name: String!
}
```

and we want to rename the `name` field:

```graphql
type User {
  id: ID! @id
  name2: String!
}
```

Prisma can guess, that this change could break the application code, but only the compiler will actually know the answer based on the new type that the Prisma Client has.

A simple `prisma generate` and build of the application will inform the user, if a change is breaking any application code.
This can be done in a fast local development loop without the need of the migration engine.

## Limits of migration inference

One of the main tasks of the migration engine is to infer the needed migration steps to accomplish the migration from datamodel A to datamodel B.
In specific use-cases, the declarative nature of the datamodel definition leaves ambiguities. Then the migration engine is not able to infer steps, that make sense. Right now only one concrete scenario causes the migration engine to not be able to infer:

If we start with this datamodel:

```graphql
type A {
  id: ID! @id
  b: B! @relation(name: "AB1")
  b2: B! @relation(name: "AB2")
}

type B {
  id: ID! @id
  a: A! @relation(name: "AB1")
  a2: A! @relation(name: "AB2")
}
```

And change it to this datamodel:

```graphql
type A {
  id: ID! @id
  b3: B!
}

type B {
  id: ID! @id
  a3: A!
}
```

Prisma won't be able to infer, if the relation `AB1` should be reused, the relation `AB2` reused or if a new relation should be created.

## Squashing & Snapshots

Over the course of months and years, the number of migrations can grow to a couple of hundred.
There may be many redundant migrations, which cancel each other out, as first removing a field and then creating it again.
When in a CI environment it can be useful to initialize a new Prisma project from scratch in an empty database.
This process should be as fast a possible though and could become slow with multiple hundred migration files.

This is an inherent problem of all file-based migrations systems and several solutions already exist in other migration systems.
The [Django Migration System](https://docs.djangoproject.com/en/2.1/topics/migrations/#migration-squashing) chooses squashing of migration files to accomplish this optimization. As we have our migration files in lockstep with the migration table though (which gives us many guarantees), this approach is not feasible with this migration approach.
[Active Record Migrations](https://edgeguides.rubyonrails.org/active_record_migrations.html) solves the migration performance issue by allowing to perform migrations either _based on migration files_ or a _schema snapshot file_. This snapshot-based migration also fits nicely into Prismas new migrations system. There is an important implementation detail required, though.
If one would implement this naively, the CLI could let the migration engine infer the necessary migration steps and create the migration table in the blank database with one migration, which just includes these steps.
However, if you want to continue to migrate this database with further steps that build on the existing ones, the migration engine will not allow this, as migrations stored in the migration table have to be in lockstep with migrations stored in the local migration files.
In order to have the best of both worlds, the migration engine **pretends** as if it would execute all migration steps and saves it like that to the migration table (which can be done very fast), but still just executes the minimum amount of steps needs to get to the desired datamodel.

This feature will be available with the `prisma migrate apply --snapshot` command.

### Requirements

- This is only possible with database, where no migration table exists yet.
- If the `m.execute` step is being used to seed necessary production data, this command should not be used. The CLI will warn the user about this, but the user will be able to continue nevertheless.

Therefore we recommend using the Prisma seeding system for seeding data that you later need in your production code.

## Data Migrations vs Seeding

As mentioned in the previous section, using the `m.execute` operation to migrate data should only be used when necessary.
If possible, the Prisma seeding system should be preferred, as this then works nicely with the snapshot-based migrations.

With snapshot-based migrations and seeding, we now accomplish very nice side effects. They basically emulate "forking" a database.
One can quickly set up the current state of the database using the snapshot-based migrations and seed the database with necessary data to run tests in CI.
Forks of the database based on production data are not yet part of this spec, as this is rather an infrastructure and performance problem outside of this migration spec.

Active Record deals with the same problem and therefore also recommends to [favor seeding over data migrations]([Active Record Migrations](https://edgeguides.rubyonrails.org/active_record_migrations.html) if possible.

## Rollbacks

In the case that a migration step doesn't succeed, the whole migration will be rolled back. If there are `n` unapplied migrations and the last migration fails, only exactly that one migration will be rolled back. The datamodel of the time of rollback will both be printed and the user will be able to read it in the corresponding migration datamodel file.
The idea is, that migration steps are the smallest atomic unit, while a migration is a collection of these.
A migration can only fully succeed or fail.
Rollbacked migrations don't have to be in lockstep. Otherwise it would not be possible to retry failed migrations.

## Rerunning Migrations

If a migration succeeded, it won't be possible to rerun it. However, once the migration engine decided, that a migration is valid, tries to execute it and fails during execution, it must be possible to retry that migration execution.

## Opting out of Prisma's migration system

While Prisma provides a great migration experience, users still can always opt out of the Prisma migration system through running their own migrations, re-introspecting the database and generating a new Prisma Client based on the new datamodel, that results from the introspection. A legit use-case is, that an organization already has experience and proven workflows around an existing migrations system like Rails migrations. In order to incrementally adopt, the team can keep migrating the database using Rails, while generating the Prisma datamodel based on the state of the database schema.
Prisma then has a read-only relation to the database schema, while still being able to read and write data.

<!---







End of detailed design








--->

# Non-Goals

A few more advanced concepts

# Drawbacks

- Splitting a "create required field" operation into "create optional field" + data migration is manual work

# Alternatives

# Adoption strategy

Users that are running the existing Prisma system will need to run `prisma migrate` instead of `prisma deploy`.
To opt into the file-based migrations, a single `prisma migrate generate` is sufficient.

# How we teach this

# Unresolved questions

## Database administrators of big corporations oftentimes need the pure SQL statements to perform a migration. How could the migrations system generate these SQL statements based on a migration script?

## How do migrations look like in Go?

## How could migrations integrate with Github Actions?

## How will the primary data source / source to save the migration table be configured?

## How do we handle big migrations that need to scale over millions of table entries?

This would require a more sophisticated migration system, that uses data syncing with a Ghost table, as implemented by https://github.com/github/gh-ost.

A potential API to handle these "online" changes in a big production system could look like this:

```ts
import { migrate } from 'prisma-sdk'

export default migrate(m => [
  m.createField({
    model: 'User',
    name: 'address',
    type: 'String',
    isRequired: true,
    migrationValue: '',
  }),
  m.runAsync({
    watch: {
      BeforeUserCreated: user => {
        const [firstName, lastName] = user.name.split(' ')
        user.firstName = firstName
        user.last = lastName
        return user
      },
    },
    run: async client => {
      for await (const user of client.users().$stream()) {
        await client.updateUser(user.id, 'BeforeUserCreated')
      }
    },
  }),
  m.updateField({
    model: 'User',
    name: 'address',
    type: 'String',
    isRequired: true,
  }),
])
```

## Should we block the API during migration?

Systems like Rails follow this approach. This would make migrations easier to reason about and prevent inconsistent data state. For a big production application this would however not be acceptable. This is probably something the application developer should take care of, by turning the application into maintenance mode.

## How to generate a (type-safe) client for the exec operations?

## Wow to detect ambiguous scenarios (e.g. rename)?

Is there a CLI flow for renaming fields? What does that look like?
